# Local AI Service

Ein webbasierter lokaler AI-Dienst fÃ¼r Ihren PC. Nutzen Sie Ihre eigenen AI-Modelle (Qwen, Phi-3, etc.) komplett offline und kostenlos.

## Features

- ğŸ¤– **Lokale AI-Modelle**: Nutzen Sie Hugging Face Modelle direkt auf Ihrem PC
- ğŸ’¬ **Modernes Chat-Interface**: SchÃ¶n gestaltetes UI mit GesprÃ¤chsverlauf
- ğŸ“ **Conversation History**: GesprÃ¤che speichern und fortsetzen
- ğŸ¯ **Preference Learning** (optional): Die AI lernt aus Ihren Interaktionen
- âš™ï¸ **Flexibel**: Einfaches Wechseln zwischen verschiedenen Modellen
- ğŸ”’ **100% Offline**: Keine Cloud-AbhÃ¤ngigkeiten, alles lokal
- ğŸŒ **Smart Browser Tabs**: Intelligentes Tab-Management - refresht existierende Tabs statt neue zu Ã¶ffnen

## Voraussetzungen

- Python 3.10 oder hÃ¶her
- CUDA-fÃ¤hige GPU (optional, aber empfohlen fÃ¼r bessere Performance)
  - **RTX 50-Serie (Blackwell)**: BenÃ¶tigt PyTorch 2.7.0+ mit CUDA 12.8+
  - **Andere GPUs**: PyTorch mit CUDA 11.8+ oder 12.4+
- Mindestens 8GB RAM (16GB+ empfohlen)
- Genug Speicherplatz fÃ¼r die Modelle

## Installation

1. **GPU-UnterstÃ¼tzung prÃ¼fen (empfohlen):**
```bash
# Windows
scripts\check_gpu.bat

# Linux/Mac
python scripts/check_gpu.py
```
   - PrÃ¼ft ob Ihre GPU erkannt wird
   - Zeigt an, ob PyTorch CUDA-UnterstÃ¼tzung hat
   - Gibt Installationsempfehlungen basierend auf Ihrer GPU

2. **PyTorch mit CUDA installieren (falls nÃ¶tig):**
   - **RTX 50-Serie (Blackwell)**: `scripts\install_pytorch_cuda.bat` (CUDA 12.8)
   - **Falls Probleme**: `scripts\install_pytorch_nightly.bat` (Nightly mit CUDA 12.9)
   - **Andere GPUs**: Siehe Empfehlungen in `check_gpu.bat`

3. **Dependencies installieren:**
```bash
pip install -r requirements.txt
```

4. **Konfiguration prÃ¼fen:**
   - Ã–ffnen Sie `config.json` und prÃ¼fen Sie, ob die Modell-Pfade korrekt sind
   - Die Pfade sollten zu Ihren Modell-Verzeichnissen zeigen

5. **Server starten:**

   **Automatisch (empfohlen):**
   ```bash
   # Windows
   start_local_ai.bat
   ```
   - Startet automatisch Model Service und Local AI Server
   - Ã–ffnet Browser-Tabs (oder refresht existierende)
   - Zum Beenden: `stop_server.bat`

   **Manuell:**
   ```bash
   cd backend
   python main.py
   ```

6. **Frontend Ã¶ffnen:**
   - Bei automatischem Start: Browser Ã¶ffnet sich automatisch
   - Oder navigieren Sie zu `http://127.0.0.1:8000/static/index.html`
   - Model Manager: `http://127.0.0.1:8001`

## Konfiguration

### Modelle hinzufÃ¼gen

Bearbeiten Sie `config.json`:

```json
{
  "models": {
    "mein-modell": {
      "name": "Mein Modell",
      "path": "G:\\Pfad\\zum\\Modell",
      "type": "qwen2",
      "description": "Beschreibung"
    }
  }
}
```

### Standard-Modell

Setzen Sie `default_model` in `config.json` auf die ID Ihres bevorzugten Modells.

## Nutzung

1. **Modell laden**: WÃ¤hlen Sie ein Modell aus dem Dropdown in der Sidebar
2. **GesprÃ¤ch starten**: Klicken Sie auf "+ Neues GesprÃ¤ch" oder stellen Sie direkt eine Frage
3. **GesprÃ¤ch fortsetzen**: Klicken Sie auf ein GesprÃ¤ch in der Sidebar
4. **Einstellungen**: Klicken Sie auf "âš™ï¸ Einstellungen" fÃ¼r erweiterte Optionen

## Smart Browser Tab Management

Das Startskript `start_local_ai.bat` verwendet intelligentes Tab-Management:

- âœ… **Erster Start**: Ã–ffnet neue Browser-Tabs fÃ¼r Model Manager und Frontend
- ğŸ”„ **Wiederholter Start**: Refresht existierende Tabs statt neue zu Ã¶ffnen
- ğŸ§¹ **Automatisches Cleanup**: `stop_server.bat` lÃ¶scht den Tab-Status

**Vorteile:**
- Keine Tab-Flut mehr bei mehrmaligem Neustart
- Automatischer Refresh der Seiten
- Funktioniert mit allen Standard-Browsern (Chrome, Edge, Firefox)

**Mehr Informationen:** Siehe [docs/SMART_BROWSER_TABS.md](docs/SMART_BROWSER_TABS.md)

## API Endpunkte

- `GET /status` - Server-Status
- `GET /models` - VerfÃ¼gbare Modelle
- `POST /models/load` - Modell laden
- `POST /chat` - Chat-Nachricht senden
- `GET /conversations` - Alle GesprÃ¤che
- `GET /conversations/{id}` - GesprÃ¤ch laden
- `POST /conversations` - Neues GesprÃ¤ch
- `DELETE /conversations/{id}` - GesprÃ¤ch lÃ¶schen
- `GET /preferences` - PrÃ¤ferenzen anzeigen
- `POST /preferences/toggle` - Preference Learning ein/aus
- `POST /preferences/reset` - PrÃ¤ferenzen zurÃ¼cksetzen

## Projektstruktur

```
.
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                 # FastAPI Server
â”‚   â”œâ”€â”€ model_manager.py        # Modell-Verwaltung
â”‚   â”œâ”€â”€ conversation_manager.py # GesprÃ¤chsverwaltung
â”‚   â””â”€â”€ preference_learner.py   # Preference Learning
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html              # Hauptinterface
â”‚   â”œâ”€â”€ style.css               # Styling
â”‚   â””â”€â”€ app.js                  # Frontend-Logik
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ conversations/          # Gespeicherte GesprÃ¤che
â”‚   â””â”€â”€ preferences.json        # Gelernte PrÃ¤ferenzen
â”œâ”€â”€ config.json                 # Konfiguration
â”œâ”€â”€ requirements.txt            # Python Dependencies
â””â”€â”€ README.md                   # Diese Datei
```

## Troubleshooting

**GPU wird nicht erkannt / CUDA nicht verfÃ¼gbar:**
- FÃ¼hren Sie `scripts\check_gpu.bat` aus, um die Ursache zu finden
- **RTX 50-Serie (Blackwell)**: Stellen Sie sicher, dass PyTorch 2.7.0+ mit CUDA 12.8+ installiert ist
  - Verwenden Sie `scripts\install_pytorch_cuda.bat` fÃ¼r stabile Version
  - Oder `scripts\install_pytorch_nightly.bat` fÃ¼r neueste Nightly-Version
- **Fehlermeldung "sm_120 not compatible"**: Ihre PyTorch-Version unterstÃ¼tzt Blackwell nicht
  - Installieren Sie PyTorch 2.7.0+ mit CUDA 12.8+
- **Andere GPUs**: PrÃ¼fen Sie, ob NVIDIA-Treiber installiert sind (`nvidia-smi`)

**Modell lÃ¤dt nicht:**
- PrÃ¼fen Sie, ob der Pfad in `config.json` korrekt ist
- Stellen Sie sicher, dass das Modell im Hugging Face Format vorliegt
- PrÃ¼fen Sie die Logs im Terminal

**Out of Memory:**
- Verwenden Sie ein kleineres Modell
- Reduzieren Sie `max_length` in den Einstellungen
- SchlieÃŸen Sie andere Anwendungen
- PrÃ¼fen Sie, ob die GPU verwendet wird (nicht CPU)

**Langsame Antworten:**
- PrÃ¼fen Sie mit `scripts\check_gpu.bat`, ob die GPU aktiv ist
- Falls CPU verwendet wird: Installieren Sie PyTorch mit CUDA-UnterstÃ¼tzung
- Reduzieren Sie `max_length`
- Verwenden Sie ein kleineres Modell

### RTX 50-Serie (Blackwell-Architektur) - Spezielle Hinweise

Die RTX 50-Serie verwendet die neue Blackwell-Architektur (Compute Capability sm_120), die spezielle Anforderungen hat:

- **Erforderlich**: PyTorch 2.7.0 oder hÃ¶her
- **Erforderlich**: CUDA 12.8 oder hÃ¶her
- **Empfohlen**: Neueste NVIDIA-Treiber (unterstÃ¼tzen CUDA 13.1+)

Falls Sie Probleme haben:
1. PrÃ¼fen Sie mit `scripts\check_gpu.bat`, ob Blackwell erkannt wird
2. Installieren Sie PyTorch mit CUDA 12.8: `scripts\install_pytorch_cuda.bat`
3. Falls das nicht funktioniert, versuchen Sie Nightly-Builds: `scripts\install_pytorch_nightly.bat`

## Lizenz

Dieses Projekt ist fÃ¼r den persÃ¶nlichen Gebrauch gedacht.


