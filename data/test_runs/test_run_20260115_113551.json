{
  "timestamp": "2026-01-15T11:32:43.577778",
  "base_url": "http://127.0.0.1:8000",
  "conversation_id": "d346aaea-13d2-4aa5-a89a-c0db2f66adef",
  "request_timeout_seconds": 60,
  "max_run_seconds": 300,
  "tests": [
    {
      "id": "short_ok",
      "max_length": 32,
      "response_preview": "",
      "analysis": {
        "char_count": 0,
        "word_count": 0,
        "warnings": []
      }
    },
    {
      "id": "file_write_short",
      "error": "timeout_after_60s",
      "timeout": true
    },
    {
      "id": "simple_points",
      "error": "timeout_after_60s",
      "timeout": true
    },
    {
      "id": "medium_explain",
      "max_length": 192,
      "response_preview": ".",
      "analysis": {
        "char_count": 1,
        "word_count": 1,
        "warnings": []
      }
    },
    {
      "id": "long_structured",
      "max_length": 256,
      "response_preview": "**Strategien zur Vermiedung von Answer-Cropping in Lokalen Llm-Setup**\n\n### Bedeutung von Token-Buchten und Kontekstl√§nge\nToken-Buchte bezieht sich auf das maximale Anzahl an Tokens, die ein Sprachmod",
      "analysis": {
        "char_count": 1145,
        "word_count": 155,
        "warnings": []
      }
    }
  ],
  "conversation_file": "G:\\04-CODING\\Local Ai\\data\\conversations\\d346aaea-13d2-4aa5-a89a-c0db2f66adef.json"
}